import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.metrics import recall_score,precision_score,f1_score
from sklearn.metrics import plot_confusion_matrix,roc_curve,precision_recall_curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, roc_curve, auc
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from imblearn.over_sampling import SMOTE

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.ensemble import RandomForestClassifier

import xgboost as xgb  


import pubchempy as pcp

pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

df = pd.read_csv('output (1).csv', encoding='ISO-8859-1')
df

df.drop(columns=["CID","NAME","ExactMass","CanonicalSMILES","PIC50","CovalentUnitCount",'ConformerCount3D',"Charge"],inplace=True)


from sklearn.preprocessing import LabelEncoder

# Create a LabelEncoder object
le = LabelEncoder()

# Fit the encoder to the target column
le.fit(df['Activity'])

# Transform the target column
df['Activity'] = le.transform(df['Activity'])
df['Activity'].value_counts()

df.isna().sum()
df= df.dropna()

y=df["Activity"]
x=df.drop("Activity",axis=1)

from tensorflow.keras.models import Sequential
import tensorflow as tf
from tensorflow.keras.layers import Dropout, Dense, BatchNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

from sklearn.metrics import confusion_matrix, roc_auc_score
import matplotlib.pyplot as plt
import math

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import matthews_corrcoef
from imblearn.over_sampling import ADASYN
from imblearn.combine import SMOTEENN, SMOTETomek

X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=123)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=123)




# Standardize features
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)


# Apply SMOTE
smote = SMOTE(random_state=42, k_neighbors=3)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Set class weights for minority class
class_weights = {0: 5, 1: 1}

# Reinitialize model with best hyperparameters
final_model = Sequential([
    Dense(units=2000, input_dim=X_train_resampled.shape[1], activation='relu', kernel_initializer='glorot_uniform'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(units=1000, activation='relu', kernel_initializer='glorot_uniform'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(units=500, activation='relu', kernel_initializer='glorot_uniform'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(units=10, activation='relu', kernel_initializer='glorot_uniform'),
    BatchNormalization(),
    Dropout(0.1),
    Dense(1, activation='sigmoid')
])

# Compile final model
final_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
final_model.compile(optimizer=final_optimizer, loss='binary_crossentropy', metrics=['accuracy'])

  # Add early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)


# Train final model with best hyperparameters
final_model.fit(X_train_resampled, y_train_resampled,
                epochs=20,
                batch_size=256,
                validation_data=(X_val, y_val),
                callbacks=[early_stopping],
                class_weight=class_weights,  # Add this line
                verbose=1)

# Evaluate on test data
test_loss, test_accuracy = final_model.evaluate(X_test, y_test)
print("Test Accuracy:", test_accuracy)
print("Test Loss:", test_loss)

# Predict probabilities and binary classes on the test set
y_pred_prob = final_model.predict(X_test).flatten()
y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions (0 or 1)

# Calculate precision, recall, and AUC
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
auc_score = roc_auc_score(y_test, y_pred_prob)
f1 = f1_score(y_test, y_pred)

print("F1 Score:", f1)

print("Precision:", precision)
print("Recall:", recall)
print("AUC Score:", auc_score)
# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
ConfusionMatrixDisplay(conf_matrix).plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()


def f1_score_threshold(threshold=0.5):
    def f1_score(y_true, y_pred):
        y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32 to match y_pred
        y_pred = tf.cast(tf.greater(y_pred, threshold), tf.float32)
        
        tp = tf.reduce_sum(y_true * y_pred)
        precision = tp / (tf.reduce_sum(y_pred) + 1e-6)
        recall = tp / (tf.reduce_sum(y_true) + 1e-6)
        
        return 2 * precision * recall / (precision + recall + 1e-6)
    
    return f1_score

# Train model

# Evaluate model and plot metrics
plt.plot(history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'], label='val_acc')
plt.legend()
plt.title("Model Accuracy")
plt.show()

plt.plot(history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.legend()
plt.title("Model Loss")
plt.show()

# Prediction and evaluation on test set
y_pred = final_model.predict(X_test)
y_pred_classes = [1 if prob > 0.5 else 0 for prob in y_pred]

cnf_matrix = confusion_matrix(y_test, y_pred_classes)
tn, fp, fn, tp = cnf_matrix.ravel()

bacc = (tp / (tp + fn) + tn / (tn + fp)) / 2
pre = tp / (tp + fp)
rec = tp / (tp + fn)
f1 = 2 * pre * rec / (pre + rec)
# mcc = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
auc = roc_auc_score(y_test, y_pred)

print("Balanced Accuracy:", bacc)
# Assuming y_pred_prob contains probability predictions
y_pred = [1 if prob >= 0.5 else 0 for prob in y_pred_prob]
mcc = matthews_corrcoef(y_test, y_pred)
print("MCC",mcc)

# Option 1: Save the model in HDF5 format
final_model.save("best_model.h5")

# # Option 2: Save as a TensorFlow SavedModel
# final_model.save("best_saved_model", save_format="tf")

from tensorflow.keras.models import load_model

# Load model in HDF5 format
loaded_model = load_model("best_model.h5")

# # Or load TensorFlow SavedModel format
# loaded_model = load_model("best_saved_model")

from tensorflow.keras.models import load_model
import pickle


# Save model in HDF5 format
final_model.save("best_model.h5")


# Save scaler using pickle
import pickle
with open("scaler.pkl", "wb") as f:
    pickle.dump(scaler, f)
    
# Load the saved model
loaded_model = load_model("best_model.h5")
    
# Load the saved scaler
with open("scaler.pkl", "rb") as f:
    loaded_scaler = pickle.load(f)

os.listdir()

# Load new CSV data
new_data = pd.read_csv('final_Mangrove_dataset.csv', encoding='ISO-8859-1')

# # Assuming 'new_data' has the same columns (except target)
# X_new = new_data.iloc[:, :-1]  # Adjust if necessary

# Apply scaling
X_new_scaled = loaded_scaler.transform(new_data)

# Step 3: Make predictions on new data
predictions = loaded_model.predict(X_new_scaled)

# Convert probabilities to binary predictions if needed
predicted_labels = (predictions > 0.5).astype(int)

predictions = np.array(predictions).flatten().astype(int)

# Creating the DataFrame
results = pd.DataFrame({
    'Prediction': predictions
})
results

zero_count = (results['Prediction'] == 0).sum()
print("Number of rows with zero predictions:", zero_count)

results.to_csv('SMOTE_MW', index=False)

